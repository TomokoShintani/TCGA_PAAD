{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import glob\n",
    "\n",
    "from collections import deque\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\tomsh\\\\OneDrive\\\\デスクトップ\\\\研究テーマ\\\\TCGAPAADdata\\\\preprocess_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() #↓微妙に違う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd() #↑微妙に違う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/tomsh/OneDrive/デスクトップ/研究テーマ/TCGAPAADdata/preprocess_data/PAAD_Exp.csv')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(path, \"PAAD_Exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_df = pd.read_csv(\"C:/Users/tomsh/OneDrive/デスクトップ/研究テーマ/TCGAPAADdata/preprocess_data/PAAD_Exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_df.shape\n",
    "#130人分のデータしかない、、、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CELF3</th>\n",
       "      <th>ADRA1B</th>\n",
       "      <th>SPC24</th>\n",
       "      <th>FIBCD1</th>\n",
       "      <th>TEKT4</th>\n",
       "      <th>MARCO</th>\n",
       "      <th>FFAR2</th>\n",
       "      <th>SLC7A9</th>\n",
       "      <th>FASLG</th>\n",
       "      <th>GPRC5D</th>\n",
       "      <th>...</th>\n",
       "      <th>ACOX2</th>\n",
       "      <th>RAC3</th>\n",
       "      <th>HIATL2</th>\n",
       "      <th>MMP28</th>\n",
       "      <th>RAD51AP1</th>\n",
       "      <th>PLCD3</th>\n",
       "      <th>TOP2A</th>\n",
       "      <th>RIMS3</th>\n",
       "      <th>FKBP9L</th>\n",
       "      <th>NCRNA00085</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.736565</td>\n",
       "      <td>1.193008</td>\n",
       "      <td>2.336809</td>\n",
       "      <td>3.255117</td>\n",
       "      <td>1.567721</td>\n",
       "      <td>3.320215</td>\n",
       "      <td>0.631639</td>\n",
       "      <td>2.421343</td>\n",
       "      <td>1.775607</td>\n",
       "      <td>2.402260</td>\n",
       "      <td>...</td>\n",
       "      <td>2.873951</td>\n",
       "      <td>2.865187</td>\n",
       "      <td>2.778039</td>\n",
       "      <td>3.641156</td>\n",
       "      <td>2.893747</td>\n",
       "      <td>3.624310</td>\n",
       "      <td>3.374425</td>\n",
       "      <td>2.630684</td>\n",
       "      <td>2.948463</td>\n",
       "      <td>2.522492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.182000</td>\n",
       "      <td>2.204594</td>\n",
       "      <td>2.415874</td>\n",
       "      <td>1.831403</td>\n",
       "      <td>1.619840</td>\n",
       "      <td>3.282466</td>\n",
       "      <td>2.855049</td>\n",
       "      <td>1.355471</td>\n",
       "      <td>1.971368</td>\n",
       "      <td>2.287101</td>\n",
       "      <td>...</td>\n",
       "      <td>2.885201</td>\n",
       "      <td>2.719042</td>\n",
       "      <td>2.550898</td>\n",
       "      <td>3.372268</td>\n",
       "      <td>2.793141</td>\n",
       "      <td>3.526903</td>\n",
       "      <td>3.488991</td>\n",
       "      <td>2.848533</td>\n",
       "      <td>3.096868</td>\n",
       "      <td>2.906825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.231265</td>\n",
       "      <td>2.515475</td>\n",
       "      <td>2.507271</td>\n",
       "      <td>2.886731</td>\n",
       "      <td>-2.145508</td>\n",
       "      <td>3.451687</td>\n",
       "      <td>2.914352</td>\n",
       "      <td>0.293977</td>\n",
       "      <td>0.293977</td>\n",
       "      <td>2.681636</td>\n",
       "      <td>...</td>\n",
       "      <td>2.726465</td>\n",
       "      <td>3.018371</td>\n",
       "      <td>2.331522</td>\n",
       "      <td>3.610377</td>\n",
       "      <td>2.935887</td>\n",
       "      <td>3.639639</td>\n",
       "      <td>3.558751</td>\n",
       "      <td>3.151823</td>\n",
       "      <td>2.886731</td>\n",
       "      <td>3.001327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.134657</td>\n",
       "      <td>2.268355</td>\n",
       "      <td>2.367026</td>\n",
       "      <td>3.133404</td>\n",
       "      <td>1.894603</td>\n",
       "      <td>3.237331</td>\n",
       "      <td>2.594578</td>\n",
       "      <td>2.268355</td>\n",
       "      <td>2.028838</td>\n",
       "      <td>2.699508</td>\n",
       "      <td>...</td>\n",
       "      <td>2.861636</td>\n",
       "      <td>2.847244</td>\n",
       "      <td>2.663824</td>\n",
       "      <td>3.519339</td>\n",
       "      <td>3.059547</td>\n",
       "      <td>3.558037</td>\n",
       "      <td>3.544204</td>\n",
       "      <td>2.699508</td>\n",
       "      <td>3.034963</td>\n",
       "      <td>2.639873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.915137</td>\n",
       "      <td>2.619283</td>\n",
       "      <td>2.228052</td>\n",
       "      <td>2.549654</td>\n",
       "      <td>0.137550</td>\n",
       "      <td>3.158111</td>\n",
       "      <td>2.094228</td>\n",
       "      <td>1.424921</td>\n",
       "      <td>2.188869</td>\n",
       "      <td>2.418993</td>\n",
       "      <td>...</td>\n",
       "      <td>2.853863</td>\n",
       "      <td>2.683018</td>\n",
       "      <td>2.228052</td>\n",
       "      <td>3.495372</td>\n",
       "      <td>2.895283</td>\n",
       "      <td>3.527122</td>\n",
       "      <td>3.429515</td>\n",
       "      <td>2.888272</td>\n",
       "      <td>3.292343</td>\n",
       "      <td>2.873591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3.038540</td>\n",
       "      <td>2.603369</td>\n",
       "      <td>2.330913</td>\n",
       "      <td>2.477391</td>\n",
       "      <td>2.256251</td>\n",
       "      <td>2.685849</td>\n",
       "      <td>0.107161</td>\n",
       "      <td>2.160066</td>\n",
       "      <td>2.303058</td>\n",
       "      <td>2.112690</td>\n",
       "      <td>...</td>\n",
       "      <td>2.704869</td>\n",
       "      <td>3.004575</td>\n",
       "      <td>2.673743</td>\n",
       "      <td>3.280016</td>\n",
       "      <td>3.031232</td>\n",
       "      <td>3.424883</td>\n",
       "      <td>3.536979</td>\n",
       "      <td>3.067876</td>\n",
       "      <td>3.027483</td>\n",
       "      <td>3.044738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3.141908</td>\n",
       "      <td>1.613815</td>\n",
       "      <td>1.504989</td>\n",
       "      <td>3.248107</td>\n",
       "      <td>1.504989</td>\n",
       "      <td>2.514170</td>\n",
       "      <td>2.324277</td>\n",
       "      <td>1.364666</td>\n",
       "      <td>2.654814</td>\n",
       "      <td>2.256050</td>\n",
       "      <td>...</td>\n",
       "      <td>2.973275</td>\n",
       "      <td>2.931361</td>\n",
       "      <td>2.353673</td>\n",
       "      <td>3.361543</td>\n",
       "      <td>2.773621</td>\n",
       "      <td>3.513814</td>\n",
       "      <td>3.223943</td>\n",
       "      <td>2.937843</td>\n",
       "      <td>2.692139</td>\n",
       "      <td>3.038970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2.597518</td>\n",
       "      <td>2.417896</td>\n",
       "      <td>1.836454</td>\n",
       "      <td>1.658234</td>\n",
       "      <td>2.377349</td>\n",
       "      <td>3.708627</td>\n",
       "      <td>2.330774</td>\n",
       "      <td>1.206832</td>\n",
       "      <td>2.130495</td>\n",
       "      <td>1.206832</td>\n",
       "      <td>...</td>\n",
       "      <td>2.975057</td>\n",
       "      <td>2.963500</td>\n",
       "      <td>2.445142</td>\n",
       "      <td>3.481236</td>\n",
       "      <td>2.747232</td>\n",
       "      <td>3.489073</td>\n",
       "      <td>3.291381</td>\n",
       "      <td>2.673257</td>\n",
       "      <td>2.837182</td>\n",
       "      <td>2.973928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2.672047</td>\n",
       "      <td>2.792286</td>\n",
       "      <td>2.138718</td>\n",
       "      <td>3.336379</td>\n",
       "      <td>1.897485</td>\n",
       "      <td>2.826775</td>\n",
       "      <td>1.712888</td>\n",
       "      <td>1.897485</td>\n",
       "      <td>1.495044</td>\n",
       "      <td>2.675498</td>\n",
       "      <td>...</td>\n",
       "      <td>2.485452</td>\n",
       "      <td>2.710759</td>\n",
       "      <td>2.306852</td>\n",
       "      <td>3.544067</td>\n",
       "      <td>2.955197</td>\n",
       "      <td>3.692108</td>\n",
       "      <td>3.394474</td>\n",
       "      <td>2.813153</td>\n",
       "      <td>2.749669</td>\n",
       "      <td>3.065801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.249175</td>\n",
       "      <td>2.349201</td>\n",
       "      <td>2.195614</td>\n",
       "      <td>0.826591</td>\n",
       "      <td>2.489233</td>\n",
       "      <td>3.287203</td>\n",
       "      <td>2.410832</td>\n",
       "      <td>1.129961</td>\n",
       "      <td>2.649843</td>\n",
       "      <td>2.305508</td>\n",
       "      <td>...</td>\n",
       "      <td>2.629596</td>\n",
       "      <td>2.216597</td>\n",
       "      <td>2.432420</td>\n",
       "      <td>3.397427</td>\n",
       "      <td>3.056204</td>\n",
       "      <td>3.290530</td>\n",
       "      <td>3.553035</td>\n",
       "      <td>2.714480</td>\n",
       "      <td>2.583972</td>\n",
       "      <td>2.654677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CELF3    ADRA1B     SPC24    FIBCD1     TEKT4     MARCO     FFAR2  \\\n",
       "0    2.736565  1.193008  2.336809  3.255117  1.567721  3.320215  0.631639   \n",
       "1    3.182000  2.204594  2.415874  1.831403  1.619840  3.282466  2.855049   \n",
       "2    3.231265  2.515475  2.507271  2.886731 -2.145508  3.451687  2.914352   \n",
       "3   -0.134657  2.268355  2.367026  3.133404  1.894603  3.237331  2.594578   \n",
       "4    2.915137  2.619283  2.228052  2.549654  0.137550  3.158111  2.094228   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "125  3.038540  2.603369  2.330913  2.477391  2.256251  2.685849  0.107161   \n",
       "126  3.141908  1.613815  1.504989  3.248107  1.504989  2.514170  2.324277   \n",
       "127  2.597518  2.417896  1.836454  1.658234  2.377349  3.708627  2.330774   \n",
       "128  2.672047  2.792286  2.138718  3.336379  1.897485  2.826775  1.712888   \n",
       "129  0.249175  2.349201  2.195614  0.826591  2.489233  3.287203  2.410832   \n",
       "\n",
       "       SLC7A9     FASLG    GPRC5D  ...     ACOX2      RAC3    HIATL2  \\\n",
       "0    2.421343  1.775607  2.402260  ...  2.873951  2.865187  2.778039   \n",
       "1    1.355471  1.971368  2.287101  ...  2.885201  2.719042  2.550898   \n",
       "2    0.293977  0.293977  2.681636  ...  2.726465  3.018371  2.331522   \n",
       "3    2.268355  2.028838  2.699508  ...  2.861636  2.847244  2.663824   \n",
       "4    1.424921  2.188869  2.418993  ...  2.853863  2.683018  2.228052   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "125  2.160066  2.303058  2.112690  ...  2.704869  3.004575  2.673743   \n",
       "126  1.364666  2.654814  2.256050  ...  2.973275  2.931361  2.353673   \n",
       "127  1.206832  2.130495  1.206832  ...  2.975057  2.963500  2.445142   \n",
       "128  1.897485  1.495044  2.675498  ...  2.485452  2.710759  2.306852   \n",
       "129  1.129961  2.649843  2.305508  ...  2.629596  2.216597  2.432420   \n",
       "\n",
       "        MMP28  RAD51AP1     PLCD3     TOP2A     RIMS3    FKBP9L  NCRNA00085  \n",
       "0    3.641156  2.893747  3.624310  3.374425  2.630684  2.948463    2.522492  \n",
       "1    3.372268  2.793141  3.526903  3.488991  2.848533  3.096868    2.906825  \n",
       "2    3.610377  2.935887  3.639639  3.558751  3.151823  2.886731    3.001327  \n",
       "3    3.519339  3.059547  3.558037  3.544204  2.699508  3.034963    2.639873  \n",
       "4    3.495372  2.895283  3.527122  3.429515  2.888272  3.292343    2.873591  \n",
       "..        ...       ...       ...       ...       ...       ...         ...  \n",
       "125  3.280016  3.031232  3.424883  3.536979  3.067876  3.027483    3.044738  \n",
       "126  3.361543  2.773621  3.513814  3.223943  2.937843  2.692139    3.038970  \n",
       "127  3.481236  2.747232  3.489073  3.291381  2.673257  2.837182    2.973928  \n",
       "128  3.544067  2.955197  3.692108  3.394474  2.813153  2.749669    3.065801  \n",
       "129  3.397427  3.056204  3.290530  3.553035  2.714480  2.583972    2.654677  \n",
       "\n",
       "[130 rows x 3000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEDataset():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X.values)\n",
    "        self.y = torch.Tensor(y.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx] \n",
    "        #後でdata_loader作るときに、シャッフルした方を\"train\"にして、シャッフルしてないほうを\"test\"にして辞書にする\n",
    "        #キーからデータを取ってこれるようにgetitem関数作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_features, encoding_dim): #encoding_dimはリストで渡す\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_features, encoding_dim[0])\n",
    "        self.pool1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(encoding_dim[0], encoding_dim[1])\n",
    "        self.pool2 = nn.Dropout(0.5)\n",
    "        print\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Encoder入った\")\n",
    "        #print(\"Encoder入力のxのsizeは:{}\".format(x.shape))\n",
    "        x = torch.tanh(self.fc1(x)) #ReLUだったら入力負の時0になっちゃうからいったんtanhでやってみる。元の論文の原因これReLUにしてたからでは？\n",
    "        x = self.pool1(x)\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.pool2(x)\n",
    "        #print(\"Encoder出力のxのsizeは:{}\".format(x.shape))\n",
    "        #print(\"Encoder出た\")\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_features):\n",
    "        super().__init__()\n",
    "        self.fc3 = nn.Linear(encoding_dim[1], encoding_dim[0])\n",
    "        self.fc4 = nn.Linear(encoding_dim[0], input_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_features, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_features, encoding_dim)\n",
    "        self.decoder = Decoder(encoding_dim, input_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_func, optimizer, data_loader, n_epochs, fout, device):\n",
    "    #初期化\n",
    "    pkl_queue = deque()\n",
    "    best_loss = 100.0\n",
    "    best_epoch = 0\n",
    "    best_model_weights = model.state_dict()  # state_dict()はtorchの関数\n",
    "    since = time.time()\n",
    "    end = time.time()\n",
    "\n",
    "    print(model, \"\\n\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH:{}/{}\".format(epoch+1, n_epochs), end=\"\")\n",
    "        print(\"EPOCH:{}/{}\".format(epoch+1, n_epochs), end=\"\", file=fout)\n",
    "\n",
    "        for phase in [\"train\"]:\n",
    "            model.train(True)\n",
    "\n",
    "            #データの指定\n",
    "            data = data_loader[phase]\n",
    "\n",
    "            #初期化\n",
    "            running_loss = 0\n",
    "\n",
    "            #ミニバッチに対するループ処理\n",
    "            for _, (data_train, target_train) in enumerate(data):\n",
    "                optimizer.zero_grad()\n",
    "                x = data_train.to(device)\n",
    "                y = target_train.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    y_pred = model(x)\n",
    "                    loss = loss_func(y_pred, y)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            epoch_loss = running_loss / (len(data)/len(x))\n",
    "\n",
    "            #最も損失が小さかったモデルを保存\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_weights, \"{}_epoch{}.pkl\".format(fout.name.split(\".txt\")[0], epoch+1))\n",
    "                pkl_queue.append(\"{}_epoch{}.pkl\".format(fout.name.split(\".txt\")[0], epoch+1))\n",
    "\n",
    "                if len(pkl_queue) > 1:\n",
    "                    pkl_file = pkl_queue.popleft()\n",
    "                    os.remove(pkl_file)\n",
    "            \n",
    "            #予測の出力\n",
    "            print(\", {}Loss:{:.4f}, Time:{:.4f}\".format(phase, epoch_loss, time.time()-end), end=\"\")\n",
    "            print(\", {}Loss:{:.4f}, Time:{:.4f}\".format(phase, epoch_loss, time.time()-end), end=\"\", file=fout)\n",
    "            print(\"\\n\", end=\"\")\n",
    "            print(\"\\n\", end=\"\", file=fout)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "    #トレーニング結果の表示\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"\\nTraining completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best loss: {:.4f} at epoch {}\".format(best_loss, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPOCH-Lossグラフの描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(file, name):\n",
    "\n",
    "    #トレーニングログファイルの読み込み\n",
    "    df = pd.read_csv(file, header=None, sep=r\"\\s+\")\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].str.replace('[A-Za-z]+:', '', regex=True)\n",
    "        df[col] = df[col].str.replace(',', '', regex=True)    \n",
    "\n",
    "    #線グラフを作成\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(range(len(df)), df.iloc[:, 1].astype('float'))\n",
    "    ax.set_title(f\"MSE loss for \\n{file}\")\n",
    "    ax.set_xlabel(\"EPOCHS\")\n",
    "    ax.set_ylabel(\"MSE loss\")\n",
    "    fig.savefig(f\"{name}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7.0651\n",
       "1     5.0733\n",
       "2     4.1891\n",
       "3     3.9501\n",
       "4     3.7942\n",
       "5     3.7198\n",
       "6     3.6719\n",
       "7     3.6438\n",
       "8     3.6235\n",
       "9     3.6026\n",
       "10    3.5959\n",
       "11    3.5841\n",
       "12    3.5834\n",
       "13    3.5829\n",
       "14    3.5706\n",
       "15    3.5682\n",
       "16    3.5639\n",
       "17    3.5593\n",
       "18    3.5637\n",
       "19    3.5599\n",
       "Name: Loss, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_temp = pd.read_csv(str(Path(path, 'AutoEncoder', 'AE_lr8e-05_epochs20_batch1.txt')), header=None, sep=r\"\\s+\")\n",
    "df_temp.columns = ['Epoch', 'Loss', 'Time']\n",
    "df_temp\n",
    "for col in df_temp.columns:\n",
    "    df_temp[col] = df_temp[col].str.replace('[A-Za-z]+:', '', regex=True)\n",
    "    df_temp[col] = df_temp[col].str.replace(',', '', regex=True)\n",
    "#df_temp['Epoch'].str.replace('[A-Za-z]+:', '', regex=True)\n",
    "df_temp['Loss'].astype('float')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    #初期化\n",
    "    running_mse = 0\n",
    "    preds = []\n",
    "\n",
    "    data = data_loader[\"test\"]\n",
    "    model.eval()\n",
    "\n",
    "    for data_test, target_test in data:\n",
    "        x = data_test.to(device)\n",
    "        y = target_test.to(device)\n",
    "\n",
    "        #予測\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "\n",
    "            #MSEの計算\n",
    "            sq_loss = ((y_pred - y)*(y_pred - y)).sum().data\n",
    "            running_mse += sq_loss\n",
    "\n",
    "            preds.append(y_pred[0])\n",
    "    #予測スコアを表示\n",
    "    preds = np.vstack(preds)\n",
    "    mse = math.sqrt(running_mse / len(data))\n",
    "    print(\"MSE: {}\".format(mse))\n",
    "\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 30\n",
    "lr = 0.000006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fileがなかった時に新しくそのディレクトリにfileを作る関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(filename):\n",
    "    if not Path(filename).is_dir():\n",
    "        Path(filename).parents[0].mkdir(parents=True, exist_ok=True)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (fc1): Linear(in_features=3000, out_features=200, bias=True)\n",
      "    (pool1): Dropout(p=0.5, inplace=False)\n",
      "    (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
      "    (pool2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc3): Linear(in_features=10, out_features=200, bias=True)\n",
      "    (fc4): Linear(in_features=200, out_features=3000, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "EPOCH:1/30, trainLoss:8.0845, Time:3.0073\n",
      "EPOCH:2/30, trainLoss:7.8724, Time:2.7945\n",
      "EPOCH:3/30, trainLoss:7.6398, Time:2.6150\n",
      "EPOCH:4/30, trainLoss:7.4287, Time:1.7154\n",
      "EPOCH:5/30, trainLoss:7.1911, Time:1.7403\n",
      "EPOCH:6/30, trainLoss:7.0279, Time:1.8954\n",
      "EPOCH:7/30, trainLoss:6.8097, Time:1.8986\n",
      "EPOCH:8/30, trainLoss:6.6263, Time:2.0944\n",
      "EPOCH:9/30, trainLoss:6.3969, Time:1.8039\n",
      "EPOCH:10/30, trainLoss:6.2471, Time:1.9358\n",
      "EPOCH:11/30, trainLoss:6.0607, Time:2.8854\n",
      "EPOCH:12/30, trainLoss:5.9337, Time:2.1424\n",
      "EPOCH:13/30, trainLoss:5.7240, Time:2.0108\n",
      "EPOCH:14/30, trainLoss:5.6028, Time:1.7036\n",
      "EPOCH:15/30, trainLoss:5.4849, Time:1.9089\n",
      "EPOCH:16/30, trainLoss:5.3240, Time:1.9554\n",
      "EPOCH:17/30, trainLoss:5.2089, Time:1.6724\n",
      "EPOCH:18/30, trainLoss:5.1115, Time:2.1007\n",
      "EPOCH:19/30, trainLoss:4.9446, Time:2.1252\n",
      "EPOCH:20/30, trainLoss:4.8783, Time:1.9609\n",
      "EPOCH:21/30, trainLoss:4.8113, Time:2.1143\n",
      "EPOCH:22/30, trainLoss:4.7481, Time:2.6301\n",
      "EPOCH:23/30, trainLoss:4.7122, Time:1.9249\n",
      "EPOCH:24/30, trainLoss:4.6263, Time:1.8362\n",
      "EPOCH:25/30, trainLoss:4.5031, Time:2.1531\n",
      "EPOCH:26/30, trainLoss:4.5167, Time:2.0268\n",
      "EPOCH:27/30, trainLoss:4.4814, Time:1.9747\n",
      "EPOCH:28/30, trainLoss:4.3345, Time:1.8787\n",
      "EPOCH:29/30, trainLoss:4.3334, Time:2.0923\n",
      "EPOCH:30/30, trainLoss:4.2572, Time:1.9072\n",
      "\n",
      "Training completed in 1m 3s\n",
      "Best loss: 4.2572 at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12487 (\\N{KATAKANA LETTER DE}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12473 (\\N{KATAKANA LETTER SU}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12463 (\\N{KATAKANA LETTER KU}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12488 (\\N{KATAKANA LETTER TO}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12483 (\\N{KATAKANA LETTER SMALL TU}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12503 (\\N{KATAKANA LETTER PU}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 30740 (\\N{CJK UNIFIED IDEOGRAPH-7814}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 31350 (\\N{CJK UNIFIED IDEOGRAPH-7A76}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12486 (\\N{KATAKANA LETTER TE}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12540 (\\N{KATAKANA-HIRAGANA PROLONGED SOUND MARK}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n",
      "C:\\Users\\tomsh\\AppData\\Local\\Temp\\ipykernel_30352\\1863318446.py:15: UserWarning: Glyph 12510 (\\N{KATAKANA LETTER MA}) missing from current font.\n",
      "  fig.savefig(f\"{name}.png\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 108.80584054865805\n",
      "[[0.8751121  0.84042287 0.8568561  ... 0.78213936 0.8633665  0.8928007 ]\n",
      " [0.87512285 0.84043795 0.85685384 ... 0.7821438  0.8633786  0.8928021 ]\n",
      " [0.87513167 0.84043646 0.85682744 ... 0.78213274 0.86338323 0.892797  ]\n",
      " ...\n",
      " [0.875128   0.8404415  0.8568415  ... 0.78214085 0.8633823  0.8927984 ]\n",
      " [0.87512946 0.84044254 0.8568485  ... 0.7821392  0.8633458  0.89279187]\n",
      " [0.8751624  0.8404688  0.8568187  ... 0.7821458  0.86338115 0.89279985]]\n",
      "[[ 0.99543417  0.9920536   0.99684364 ... -0.99696326 -0.99645925\n",
      "  -0.9925035 ]\n",
      " [ 0.99543864  0.99198276  0.9968214  ... -0.99693364 -0.9964385\n",
      "  -0.99244887]\n",
      " [ 0.99554425  0.99182874  0.9966045  ... -0.997023   -0.99642766\n",
      "  -0.9926946 ]\n",
      " ...\n",
      " [ 0.99553496  0.991913    0.99661726 ... -0.9970293  -0.99643904\n",
      "  -0.9926074 ]\n",
      " [ 0.9951535   0.99243635  0.99682254 ... -0.99675065 -0.9962982\n",
      "  -0.9922765 ]\n",
      " [ 0.99533224  0.99195254  0.99659693 ... -0.9969098  -0.9963631\n",
      "  -0.99257326]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    path = Path.cwd()\n",
    "\n",
    "    #input_dataの読み込み\n",
    "    df = pd.read_csv(Path(path, \"PAAD_Exp.csv\"))\n",
    "    X = df.astype(np.float32)\n",
    "\n",
    "    #load AutoEncoder\n",
    "    model = AutoEncoder(3000, [200, 10])\n",
    "    model = model.to(device)\n",
    "\n",
    "    #dataset 作る\n",
    "    AEdata = AEDataset(X, X)\n",
    "    train_data = DataLoader(AEdata, batch_size=batch_size, shuffle=True)\n",
    "    test_data = DataLoader(AEdata, batch_size=batch_size, shuffle=False)\n",
    "    data_loader = {\"train\": train_data, \"test\" : test_data}\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    #トレーニングログファイルの設定\n",
    "    train_log = \"AE_lr{}_epochs{}_batch{}.txt\".format(lr, n_epochs, batch_size)\n",
    "    fout = open(check_path(str(Path(path, \"AutoEncoder\", train_log))), \"w\") #ログファイルを書き込みモードで開く\n",
    "\n",
    "    #train AutoEncoder\n",
    "    train_model(model, loss_func, optimizer, data_loader, n_epochs, fout, device)\n",
    "\n",
    "    fout.close() #書き込み終了\n",
    "\n",
    "    #train lossの書き出し\n",
    "    plot_loss(str(Path(path, \"AutoEncoder\", train_log)), check_path(str(Path(path, \"AutoEncoder\", f\"epoch{n_epochs}_loss\"))))\n",
    "\n",
    "    #最も性能の良いAEモデルの読み込み\n",
    "    trained_model = glob.glob(str(Path(path, \"AutoEncoder\", \"{}_*.pkl\".format(train_log.split(\".txt\")[0]))))[0]\n",
    "    model.load_state_dict(torch.load(trained_model), strict=False)\n",
    "\n",
    "    #学習済みモデルを用いた予測\n",
    "    decoded_result = eval_model(model, data_loader, device)\n",
    "    print(decoded_result)\n",
    "\n",
    "    #もっともよい学習済みモデルからの特徴抽出\n",
    "    bottleneck_features = model.encoder(torch.Tensor(X.values)).detach().numpy()\n",
    "    print(bottleneck_features)\n",
    "\n",
    "    #低次元特徴量の保存\n",
    "    np.savetxt(check_path(str(Path(path, \"Bottleneck\", f\"epoch{n_epochs}_std_Exp.csv\"))), bottleneck_features, delimiter=\",\")\n",
    "    np.save(check_path(str(Path(path, \"Bottleneck\", f\"epoch{n_epochs}_std_Exp.npy\"))), bottleneck_features)\n",
    "\n",
    "    #decodeしたデータの保存\n",
    "    np.savetxt(check_path(str(Path(path, \"DecodedExp\", f\"epoch{n_epochs}_std_Exp.csv\"))), decoded_result, delimiter=\",\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass LinearReLU(nn.Module):\\n    def __init__(self, input_dim, output_dim):\\n        super(LinearReLU, self).__init__()\\n        self.f = nn.Sequential(\\n            nn.Linear(input_dim, output_dim),\\n            nn.BatchNorm1d(output_dim),\\n            nn.ReLU(True)\\n        )\\n    \\n    def forward(self, x):\\n        h = self,f(x)\\n        return(h)\\n    \\nclass Encoder(nn.Module):\\n    def __init__(self, bottle_dim)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class LinearReLU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearReLU, self).__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self,f(x)\n",
    "        return(h)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, bottle_dim)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
